{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gensim, word2vec\n",
    "\n",
    "Gensim - вообще библиотека для тематического моделирования текстов. Один из компонентов в ней - реализация на python алгоритмов из библиотеки word2vec (которая в оригинале была написана на C++).\n",
    "\n",
    "Если gensim у вас не стоит, то ставим:\n",
    "\n",
    "`pip install gensim`\n",
    "\n",
    "Поскольку иногда тренировка модели занимает много времени, то можно ещё вести лог событий."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ancatmara\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import gensim, logging\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Процесс тренировки модели \n",
    "\n",
    "На вход модели даем текстовый файл, каждое предложение на отдельной строчке. \n",
    "\n",
    "**NB!** Обратите внимание, что тренировка модели не включает препроцессинг! Это значит, что избавляться от пунктуации, приводить слова к нижнему регистру, лемматизировать их, проставлять частеречные теги придется до тренировки модели (если, конечно, это необходимо для вашей задачи). Т.е. в каком виде слова будут в исходном тексте, в таком они будут и в модели.\n",
    "\n",
    "Возьмем, например, \"Бедную Лизу\". В этом тексте заранее убрана пунтуация и слова приведены к нижнему регистру, но не лемматизированы и не размечены по частям речи."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = 'liza.txt'\n",
    "data = gensim.models.word2vec.LineSentence(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Инициализируем модель. Параметры в скобочках:\n",
    "* data - данные, \n",
    "* size - размер вектора, \n",
    "* window - размер окна наблюдения,\n",
    "* min_count - мин. частотность слова в корпусе, которое мы берем,\n",
    "* sg - используемый алгоритм обучение (0 - CBOW, 1 - Skip-gram))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-28 09:58:55,047 : INFO : collecting all words and their counts\n",
      "2018-05-28 09:58:55,049 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2018-05-28 09:58:55,058 : INFO : collected 2025 word types from a corpus of 5045 raw words and 395 sentences\n",
      "2018-05-28 09:58:55,059 : INFO : Loading a fresh vocabulary\n",
      "2018-05-28 09:58:55,064 : INFO : min_count=2 retains 609 unique words (30% of original 2025, drops 1416)\n",
      "2018-05-28 09:58:55,066 : INFO : min_count=2 leaves 3629 word corpus (71% of original 5045, drops 1416)\n",
      "2018-05-28 09:58:55,071 : INFO : deleting the raw counts dictionary of 2025 items\n",
      "2018-05-28 09:58:55,073 : INFO : sample=0.001 downsamples 63 most-common words\n",
      "2018-05-28 09:58:55,076 : INFO : downsampling leaves estimated 2611 word corpus (72.0% of prior 3629)\n",
      "2018-05-28 09:58:55,080 : INFO : estimated required memory for 609 words and 500 dimensions: 2740500 bytes\n",
      "2018-05-28 09:58:55,085 : INFO : resetting layer weights\n",
      "2018-05-28 09:58:55,107 : INFO : training model with 3 workers on 609 vocabulary and 500 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2018-05-28 09:58:55,131 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-05-28 09:58:55,134 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-05-28 09:58:55,140 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-05-28 09:58:55,143 : INFO : EPOCH - 1 : training on 5045 raw words (2580 effective words) took 0.0s, 84030 effective words/s\n",
      "2018-05-28 09:58:55,154 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-05-28 09:58:55,160 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-05-28 09:58:55,172 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-05-28 09:58:55,176 : INFO : EPOCH - 2 : training on 5045 raw words (2607 effective words) took 0.0s, 101009 effective words/s\n",
      "2018-05-28 09:58:55,195 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-05-28 09:58:55,197 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-05-28 09:58:55,215 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-05-28 09:58:55,216 : INFO : EPOCH - 3 : training on 5045 raw words (2610 effective words) took 0.0s, 88766 effective words/s\n",
      "2018-05-28 09:58:55,229 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-05-28 09:58:55,234 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-05-28 09:58:55,248 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-05-28 09:58:55,252 : INFO : EPOCH - 4 : training on 5045 raw words (2623 effective words) took 0.0s, 86546 effective words/s\n",
      "2018-05-28 09:58:55,268 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-05-28 09:58:55,274 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-05-28 09:58:55,284 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-05-28 09:58:55,293 : INFO : EPOCH - 5 : training on 5045 raw words (2601 effective words) took 0.0s, 76405 effective words/s\n",
      "2018-05-28 09:58:55,294 : INFO : training on a 25225 raw words (13021 effective words) took 0.2s, 70482 effective words/s\n",
      "2018-05-28 09:58:55,297 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 255 ms\n"
     ]
    }
   ],
   "source": [
    "%time model = gensim.models.Word2Vec(data, size=500, window=10, min_count=2, sg=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно нормализовать вектора, тогда модель будет занимать меньше RAM. Однако после этого её нельзя дотренировывать."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-28 09:59:00,403 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    }
   ],
   "source": [
    "model.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Смотрим, сколько в модели слов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "609\n"
     ]
    }
   ],
   "source": [
    "print(len(model.wv.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['может', 'быть', 'никто', 'из', 'в', 'москве', 'не', 'знает', 'так', 'хорошо', 'города', 'сего', 'как', 'я', 'потому', 'что', 'чаще', 'моего', 'бывает', 'поле', 'более', 'без', 'куда', 'глаза', 'по', 'лугам', 'и', 'новые', 'места', 'или', 'красоты', 'но', 'всего', 'для', 'меня', 'то', 'место', 'мрачные', 'синова', 'монастыря', 'на', 'сей', 'горе', 'стороне', 'почти', 'всю', 'москву', 'сию', 'которая', 'великолепная', 'картина', 'когда', 'нее', 'солнце', 'лучи', 'его', 'бесчисленных', 'к', 'небу', 'луга', 'а', 'за', 'ними', 'под', 'которые', 'от', 'российской', 'хлебом', 'другой', 'реки', 'подле', 'там', 'тению', 'песни', 'тем', 'дни', 'столь', 'них', 'древних', 'еще', 'далее', 'же', 'с', 'своим', 'часто', 'прихожу', 'сие', 'всегда', 'вместе', 'страшно', 'между', 'травою', 'келий', 'опершись', 'стону', 'времен', 'которого', 'сердце', 'мое', 'иногда', 'себе', 'тех', 'здесь', 'перед', 'о', 'своих', 'ибо', 'все', 'него', 'жизни', 'чувства', 'кроме', 'лицом', 'взором', 'сквозь', 'видит', 'свободно', 'море', 'слезы', 'глаз', 'он', 'мне', 'смерть', 'сем', 'тут', 'образ', 'моей', 'памяти', 'историю', 'отечества', 'печальную', 'несчастная', 'бога', 'воспоминание', 'судьбе', 'лизы', 'ах', 'люблю', 'те', 'проливать', 'нежной', 'скорби', 'березовой', 'хижина', 'давно', 'этой', 'хижине', 'лет', 'сим', 'жила', 'прекрасная', 'любезная', 'лиза', 'матерью', 'своею', 'отец', 'был', 'довольно', 'любил', 'работу', 'землю', 'вел', 'жизнь', 'скоро', 'смерти', 'дочь', 'рука', 'худо', 'они', 'были', 'свою', 'весьма', 'деньги', 'тому', 'бедная', 'мужа', 'своего', 'любить', 'день', 'дня', 'совсем', 'могла', 'работать', 'одна', 'осталась', 'после', 'отца', 'щадя', 'своей', 'молодости', 'ночь', 'чулки', 'рвала', 'цветы', 'брала', 'продавала', 'их', 'чувствительная', 'добрая', 'старушка', 'видя', 'дочери', 'ее', 'сердцу', 'чтобы', 'она', 'матери', 'бог', 'руки', 'говорила', 'ты', 'мною', 'была', 'теперь', 'пришла', 'моя', 'ходить', 'тобою', 'перестань', 'только', 'плакать', 'нежная', 'слез', 'у', 'старалась', 'таить', 'том', 'свете', 'отвечала', 'перестану', 'будут', 'верно', 'буду', 'твоего', 'хочу', 'умереть', 'будет', 'кого', 'тебя', 'нет', 'дай', 'прежде', 'добрый', 'человек', 'тогда', 'милых', 'моих', 'спокойно', 'прошло', 'два', 'лизина', 'цветами', 'молодой', 'ей', 'улице', 'ему', 'закраснелась', 'девушка', 'улыбкою', 'тебе', 'надобно', 'пять', 'копеек', 'это', 'вот', 'взглянуть', 'молодого', 'человека', 'сказала', 'возьмет', 'рубля', 'чего', 'лишнего', 'думаю', 'хотел', 'бы', 'взяла', 'поклонилась', 'хотела', 'идти', 'незнакомец', 'руку', 'домой', 'где', 'дом', 'твой', 'пошла', 'того', 'нею', 'случилось', 'сделала', 'матушка', 'этого', 'такое', 'такой', 'голос', 'однако', 'ж', 'своими', 'ничего', 'знаешь', 'друг', 'мои', 'люди', 'могут', 'бедную', 'девушку', 'своем', 'город', 'господа', 'глазах', 'мать', 'опять', 'тихонько', 'купить', 'смотрела', 'другую', 'вечер', 'надлежало', 'возвратиться', 'грусть', 'ввечеру', 'сидела', 'окном', 'тихим', 'голосом', 'вдруг', 'закричала', 'стоял', 'сделалось', 'спросила', 'увидела', 'который', 'старуха', 'окно', 'таким', 'приятным', 'видом', 'об', 'нем', 'здравствуй', 'сказал', 'очень', 'ли', 'знала', 'побежала', 'чистым', 'схватила', 'сама', 'всякий', 'благодарил', 'лизу', 'взорами', 'проч', 'слушал', 'со', 'сказывать', 'молния', 'обращались', 'земле', 'хотелось', 'твоя', 'образом', 'будешь', 'расставаться', 'сам', 'могу', 'лизиных', 'блеснула', 'радость', 'которую', 'заря', 'свой', 'лизой', 'бывают', 'отменно', 'уже', 'да', 'нам', 'ласковый', 'барин', 'эрастом', 'отвечал', 'раз', 'имя', 'эраст', 'до', 'свидания', 'глазами', 'задумчивости', 'если', 'жених', 'этому', 'статься', 'читатель', 'должен', 'знать', 'добрым', 'сердцем', 'природы', 'думал', 'удовольствии', 'искал', 'находил', 'при', 'воображение', 'времена', 'верить', 'свои', 'казалось', 'лизе', 'натура', 'объятия', 'решился', 'время', 'оставить', 'свет', 'души', 'минуту', 'просыпалась', 'вздыхала', 'встала', 'берег', 'траве', 'подгорюнившись', 'блестящие', 'натуры', 'лучами', 'света', 'времени', 'утром', 'душа', 'твоих', 'пастух', 'берегу', 'гнал', 'стадо', 'играя', 'свирели', 'взор', 'думала', 'тот', 'кто', 'мысли', 'мимо', 'свое', 'любезный', 'взглянул', 'ласковым', 'взял', 'мечта', 'скрылся', 'услышала', 'шум', 'эраста', 'ней', 'конечно', 'стояла', 'поцеловал', 'вся', 'милая', 'сии', 'слова', 'во', 'едва', 'смела', 'любим', 'много', 'другу', 'говорили', 'часа', 'им', 'наконец', 'можешь', 'этом', 'верю', 'ведь', 'нельзя', 'счастлива', 'говорить', 'ни', 'хотя', 'последний', 'видеться', 'роще', 'близ', 'лизиной', 'хижины', 'сто', 'вслед', 'таком', 'каком', 'вышла', 'лице', 'всех', 'любит', 'утро', 'весело', 'никогда', 'сияло', 'оно', 'самом', 'деле', 'шестой', 'наглядеться', 'небо', 'год', 'царь', 'мы', 'душу', 'скорее', 'нежели', 'милого', 'друга', 'дубов', 'пруд', 'которыми', 'любви', 'поцелуем', 'мой', 'своему', 'себя', 'забываю', 'чудно', 'жить', 'лизою', 'есть', 'чувств', 'добра', 'всякого', 'видела', 'ним', 'встретилась', 'милым', 'десять', 'дороже', 'несколько', 'недель', 'долго', 'плакала', 'должна', 'сказать', 'жестокий', 'спокойствия', 'милый', 'говорил', 'счастие', 'моему', 'бросилась', 'час', 'непорочности', 'боялась', 'мрак', 'одной', 'луч', 'мог', 'зная', 'боюсь', 'умею', 'гром', 'прощалась', 'будем', 'прости', 'завтра', 'увидимся', 'одними', 'одним', 'больше', 'самое', 'любовь', 'принадлежит', 'всем', 'твою', 'прощаясь', 'проститься', 'война', 'упала', 'обморок', 'потом', 'остаться', 'величайшим', 'поезжай', 'страшна', 'думать', 'надеюсь', 'береги', 'помни', 'описать', 'слыша', 'сказав', 'возвратился', 'благодарила', 'чувствовала', 'объятиях', 'душою', 'спасти', 'постой', 'один', 'карета', 'привел', 'обстоятельства', 'жениться', 'поцеловать', 'земля', 'пруда', 'анюта', 'скажи', 'могиле', 'стонет']\n"
     ]
    }
   ],
   "source": [
    "print([w for w in model.wv.vocab])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И сохраняем!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-28 10:01:53,394 : INFO : saving Word2Vec object under my.model, separately None\n",
      "2018-05-28 10:01:53,399 : INFO : not storing attribute vectors_norm\n",
      "2018-05-28 10:01:53,401 : INFO : not storing attribute cum_table\n",
      "2018-05-28 10:01:53,439 : INFO : saved my.model\n"
     ]
    }
   ],
   "source": [
    "model.save('my.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Работа с моделью\n",
    "\n",
    "Для каких-то своих индивидуальных нужд и экспериментов бывает полезно самому натренировать модель на нужных данных и с нужными параметрами. Но для каких-то общих целей модели уже есть как для русского языка, так и для английского.\n",
    "\n",
    "Модели для русского скачать можно здесь - http://rusvectores.org/ru/models\n",
    "\n",
    "Скачаем модель для русского языка, созданную на основе НКРЯ. Поскольку модели бывают разных форматов, то для них написаны разные функции загрузки; бывает полезно учитывать это в своем скрипте:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('ruscorpora_mystem_cbow_300_2_2015.bin.gz',\n",
       " <http.client.HTTPMessage at 0x21f277940f0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import urllib.request\n",
    "\n",
    "urllib.request.urlretrieve(\"http://rusvectores.org/static/models/rusvectores2/ruscorpora_mystem_cbow_300_2_2015.bin.gz\", \"ruscorpora_mystem_cbow_300_2_2015.bin.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-06-09 00:46:34,607 : INFO : loading projection weights from ruscorpora_mystem_cbow_300_2_2015.bin.gz\n",
      "2018-06-09 00:46:48,478 : INFO : loaded (281776, 300) matrix from ruscorpora_mystem_cbow_300_2_2015.bin.gz\n"
     ]
    }
   ],
   "source": [
    "m = 'ruscorpora_mystem_cbow_300_2_2015.bin.gz'\n",
    "if m.endswith('.vec.gz'):\n",
    "    model = gensim.models.KeyedVectors.load_word2vec_format(m, binary=False)\n",
    "elif m.endswith('.bin.gz'):\n",
    "    model = gensim.models.KeyedVectors.load_word2vec_format(m, binary=True)\n",
    "else:\n",
    "    model = gensim.models.KeyedVectors.load(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-06-09 00:49:51,702 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    }
   ],
   "source": [
    "model.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words = ['день_S', 'ночь_S', 'человек_S', 'семантика_S', 'студент_S']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Частеречные тэги нужны, поскольку это специфика скачанной модели - она была натренирована на словах, аннотированных их частями речи (и лемматизированных). **NB!** В названиях моделей на `rusvectores` указано, какой тегсет они используют (mystem, upos и т.д.)\n",
    "\n",
    "Попросим у модели 10 ближайших соседей для каждого слова и коэффициент косинусной близости для каждого:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "день_S\n",
      "[-0.02580778  0.00970898  0.01941961 -0.02332282  0.02017624  0.07275085\n",
      " -0.01444375  0.03316632  0.01242602  0.02833412]\n",
      "неделя_S 0.7165195941925049\n",
      "месяц_S 0.6310489177703857\n",
      "вечер_S 0.5828738808631897\n",
      "утро_S 0.5676206946372986\n",
      "час_S 0.5605547428131104\n",
      "минута_S 0.5297019481658936\n",
      "гекатомбеон_S 0.4897990822792053\n",
      "денек_S 0.48224717378616333\n",
      "полчаса_S 0.48217129707336426\n",
      "ночь_S 0.478074848651886\n",
      "\n",
      "\n",
      "ночь_S\n",
      "[-0.00688948  0.00408364  0.06975466 -0.00959525  0.0194835   0.04057068\n",
      " -0.00994112  0.06064967 -0.00522624  0.00520327]\n",
      "вечер_S 0.6946247816085815\n",
      "утро_S 0.5730193853378296\n",
      "ноченька_S 0.5582467317581177\n",
      "рассвет_S 0.5553582906723022\n",
      "ночка_S 0.5351512432098389\n",
      "полдень_S 0.5334426760673523\n",
      "полночь_S 0.478694349527359\n",
      "день_S 0.4780748784542084\n",
      "сумерки_S 0.43902185559272766\n",
      "фундерфун_S 0.4340824782848358\n",
      "\n",
      "\n",
      "человек_S\n",
      "[ 0.02013756 -0.02670704 -0.02039861 -0.05477146  0.00086402 -0.01636335\n",
      "  0.04240307 -0.00025525 -0.14045683  0.04785006]\n",
      "женщина_S 0.5979775190353394\n",
      "парень_S 0.4991787374019623\n",
      "мужчина_S 0.4767409563064575\n",
      "мужик_S 0.47383999824523926\n",
      "россиянин_S 0.4719043970108032\n",
      "народ_S 0.4654741883277893\n",
      "согражданин_S 0.45378509163856506\n",
      "горожанин_S 0.44368088245391846\n",
      "девушка_S 0.44314485788345337\n",
      "иностранец_S 0.43849870562553406\n",
      "\n",
      "\n",
      "семантика_S\n",
      "[-0.03066749  0.0053851   0.1110732   0.0152335   0.00440643  0.00384104\n",
      "  0.00096944 -0.03538784 -0.00079585  0.03220548]\n",
      "семантический_A 0.5334584712982178\n",
      "понятие_S 0.5030269026756287\n",
      "сочетаемость_S 0.4817051887512207\n",
      "актант_S 0.47596412897109985\n",
      "хронотоп_S 0.46330296993255615\n",
      "метафора_S 0.46158891916275024\n",
      "мышление_S 0.4610119163990021\n",
      "парадигма_S 0.4579665958881378\n",
      "лексема_S 0.45688074827194214\n",
      "смысловой_A 0.4543077349662781\n",
      "\n",
      "\n",
      "студент_S\n",
      "[ 0.02558023  0.0529849  -0.07036145  0.00279281 -0.09874777 -0.01620521\n",
      " -0.03918766  0.0326411   0.09191283  0.03495219]\n",
      "преподаватель_S 0.6958175897598267\n",
      "аспирант_S 0.6589953303337097\n",
      "выпускник_S 0.6523089408874512\n",
      "студентка_S 0.6321653127670288\n",
      "профессор_S 0.6080018281936646\n",
      "курсистка_S 0.5818493366241455\n",
      "юрфак_S 0.5806691646575928\n",
      "первокурсник_S 0.5805511474609375\n",
      "семинарист_S 0.5773230791091919\n",
      "гимназист_S 0.5747810006141663\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    # есть ли слово в модели? Может быть, и нет\n",
    "    if word in model:\n",
    "        print(word)\n",
    "        # смотрим на вектор слова (его размерность 300, смотрим на первые 10 чисел)\n",
    "        print(model[word][:10])\n",
    "        # выдаем 10 ближайших соседей слова:\n",
    "        for i in model.most_similar(positive=[word], topn=10):\n",
    "            # слово + коэффициент косинусной близости\n",
    "            print(i[0], i[1])\n",
    "        print('\\n')\n",
    "    else:\n",
    "        # Увы!\n",
    "        print(word + ' is not present in the model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Находим косинусную близость пары слов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.238956092849\n"
     ]
    }
   ],
   "source": [
    "print(model.similarity('человек_S', 'обезьяна_S'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Найди лишнее!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "огурец_S\n"
     ]
    }
   ],
   "source": [
    "print(model.doesnt_match('яблоко_S груша_S виноград_S банан_S апельсин_S огурец_S'.split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реши пропорцию!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "пельмень_S\n"
     ]
    }
   ],
   "source": [
    "print(model.most_similar(positive=['пицца_S', 'сибирь_S'], negative=['италия_S'])[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Это конечно все хорошо, но как понять, какая модель лучше? Или вот например я сделал свою модель, насколько она хорошая?\n",
    "\n",
    "Для этого существуют специальные датасеты для оценки качества дистрибутивных моделей. Их два: один измеряет точность решения задач на аналогии (про Россию и пельмени), а второй используется для оценки коэффициента семантической близости. Подробнее читаем тут:  \n",
    "* http://www.aclweb.org/aclwiki/index.php?title=Google_analogy_test_set_(State_of_the_art)\n",
    "* https://www.aclweb.org/aclwiki/index.php?title=SimLex-999_(State_of_the_art)\n",
    "\n",
    "Датасеты для русского языка можно скачать на странице с моделями на RusVectores. Посчитаем качество нашей модели НКРЯ на датасете про аналогии:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.accuracy('ruanalogy_upos.txt', restrict_vocab=3000000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
