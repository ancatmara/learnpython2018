{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are existing modules for parsing HTML. These are also not perfect, but they can often be a lot more perfect that you or I might have patience for;)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One very common solution in Python is **Beautiful Soup**, a free open-source module for Python for parsing and manipulating HTML. You have to install it yourself, but once installed, it can be called like any other module. The module is called **bs4** and the relevant function is `BeautifulSoup()`. What that does is parse the HTML and build a document model. This is a treelike representation of the HTML document and you can extract elements from it easily. The following code exemplifies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import for reading urls\n",
    "import urllib.request\n",
    "#import for parsing html\n",
    "from bs4 import BeautifulSoup\n",
    "#non-local page this time\n",
    "link = \"https://habr.com/\"\n",
    "#connect to that page\n",
    "f = urllib.request.urlopen(link)\n",
    "#read it all in\n",
    "myfile = f.read()\n",
    "#build a document model\n",
    "soup = BeautifulSoup(myfile,'html.parser')\n",
    "#print the page verbatim\n",
    "print(myfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we read in a web page and then parse it with `BeautifulSoup()`. We can then print a pretty version of it with `prettify()`, extract the text with `get_text()`, or find all instances of a tag with `find_all()`. Each tag found\n",
    "is its own treelike representation, so we can continue to call methods on them. In the example at hand, we call the `get()` method to extract the text of the `href` attribute for the `a` tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pretty-print the html\n",
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract the text\n",
    "print(soup.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#got through all the hyperlinks...\n",
    "for link in soup.find_all('a'):\n",
    "#...and print them\n",
    "    print(link.get('href'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An alternative module for parsing HTML is **lxml**. It is quite easy to parse the HTML code extracted with the help of lxml. As soon as we trasformed our data into a tree, we can use XPath to extract them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Яндекс\n",
      "['https://mail.yandex.ru', '//yandex.ru']\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from lxml import html\n",
    "\n",
    "response = requests.get('http://ya.ru')\n",
    "\n",
    "# Преобразование тела документа в дерево элементов (DOM)\n",
    "parsed_body = html.fromstring(response.text)\n",
    "\n",
    "# Выполнение xpath в дереве элементов\n",
    "print(parsed_body.xpath('//title/text()')[0])  # Получить title страницы\n",
    "print(parsed_body.xpath('//a/@href'))          # Получить аттрибут href для всех ссылок"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
